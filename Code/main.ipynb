{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Based on the plots and results, the values which returned the highest accuracy was considered for this script (M, La)\n",
    "#Import Required packages\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset is seperated into training, validation and test. \n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "\n",
    "# Regularization parameter to minimize error \n",
    "C_Lambda = 0.02\n",
    "\n",
    "\n",
    "#M is the number of basis function\n",
    "M = 4\n",
    "\n",
    "# φ is a vector of M basis functions\n",
    "PHI = []\n",
    "\n",
    "\n",
    "# This method reads the target file for all 69623 entries and appends the content to variable, and returns it as a list\n",
    "def GetTargetVector(filePath):\n",
    "    t = []\n",
    "    with open(filePath, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:  \n",
    "            t.append(int(row[0]))\n",
    "    return t\n",
    "\n",
    "#This method processes the raw data file for all 69623 entries and stores all feature set as np array of dimensions 41*69623\n",
    "#A matrix is built for the feature set. 5 rows are deleted since it contains all zero, and would have no significance on the clustering. \n",
    "def GenerateRawData(filePath):  \n",
    "    dataMatrix = [] \n",
    "    with open(filePath, 'r') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(float(column))\n",
    "            dataMatrix.append(dataRow)   \n",
    "    \n",
    "    #We delete the columns 5,6,7,8 and 9 because all the entries are zero\n",
    "    dataMatrix = np.delete(dataMatrix, [5,6,7,8,9], axis=1)\n",
    "    dataMatrix = np.transpose(dataMatrix)        \n",
    "    #dataMatrix = [ [0,1,.. 41] [[0,1,.. 41]].... [0,1,.. 41] ]\n",
    "    return dataMatrix\n",
    "\n",
    "#Querylevelnorm_t contains the target(t) for 69623 values\n",
    "#Querylevelnorm_X contains the target(t) for 69623 values\n",
    "RawTarget = GetTargetVector('Querylevelnorm_t.csv')\n",
    "RawData   = GenerateRawData('Querylevelnorm_X.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method gets the target value for training set(55699 entries) appends the content to variable, and returns it as a list\n",
    "# 80% of the actual data (test data) is used here, to compose Traininglength\n",
    "def GenerateTrainingTarget(rawTraining):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "# This method constructs the design matrix for training data, a matrix of 41*55699\n",
    "# 80% of the actual data (test data) is used construct the feature datamatrix\n",
    "def GenerateTrainingDataMatrix(rawData):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    t = rawData[:,0:T_len]\n",
    "    return t\n",
    "\n",
    "#The method assigns target for validation and test dataset. \n",
    "# 10% of the actual data (validation data) is used here\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize \n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    return t\n",
    "\n",
    "# This method constructs the design matrix for validation and test data\n",
    "# 10% of the actual data (validation data) is used here\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    return dataMatrix\n",
    "\n",
    "# Dataset is seperated into training, validation and test. \n",
    "# Training, validation and test is of 80, 10 and 10 percent of the actual data, with no overlap of data entries\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "\n",
    "\n",
    "#Preparing Training dataset\n",
    "# It extracts 55699 values from  0th position in the Rawtarget and stores it in an array. \n",
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget))\n",
    "#It generates a matrix of 41*55699 for training set\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData)\n",
    "\n",
    "#Preparing validating dataset\n",
    "## It extracts 6962 values from 55699th position in the Rawtarget and stores it in an in the array.  \n",
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget))))\n",
    "#It generates a matrix of 41*6962 for validation set\n",
    "ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget)))\n",
    "\n",
    "#Preparing Testing dataset\n",
    "# It extracts 6961 values from 62661th position in the Rawtarget and stores it in an in the array.  \n",
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "#It generates a matrix of 41*6961 for validation set\n",
    "TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Kmeans cluster stores M centroids\n",
    "#We have M number of individual clusters, and M centroids, with each  \n",
    "#Mu contains centroids for each basis function, containing detail about all 41 features  \n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData))\n",
    "#Mu is the cordinate of centroid for each cluster\n",
    "#Mu is an array of size of (M, 41)\n",
    "Mu = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This method generates the variance matrix. It contains \n",
    "#So in order to make matrix multiplication feasible, we expand the variance vector (varVect)\n",
    "#Only the diagonal is filled and rest is zero entries. \n",
    "#Thus the bigsigma contains only diagonal entries (variance) and the rest are zero (covariance).\n",
    "\n",
    "def GenerateBigSigma(Data,TrainingPercent):\n",
    "    #Returns a 41*41 matrix filled with zeros\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    #Calculate the transpose of RawData, to get matrix of dimensions 69623*41\n",
    "    DataT       = np.transpose(Data)\n",
    "    # The Training dataset entries is 55699\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))    \n",
    "    #Varvect is the variance vector of dimension 1*41\n",
    "    #This is expanded to Bigsigma of dimensions 41*41\n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])\n",
    "            #Returns the variance of the array elements\n",
    "        varVect.append(np.var(vct))\n",
    "    #Varvect contains the variance and is an list of 41 elements (matrix of 1*41)\n",
    "    #Bigsigma is 41*41 matrix, extended from varvect. Bigsigma only has diagonal filled and rest is zero\n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    BigSigma = np.dot(100,BigSigma)\n",
    "    #Bigsigma returns the variance matrix\n",
    "    return BigSigma\n",
    "\n",
    "#Variance matrix is found using Bigsigma method \n",
    "#Variance being a scalar, we generate a Matrix by multiplying variance value for each feature into Identity vector\n",
    "#Bigsigma is 41*41 matrix, extended from varvect. Bigsigma only has diagonal filled and rest is zero\n",
    "BigSigma     = GenerateBigSigma(RawData, TrainingPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the scalar product of Gaussian radial basis functions\n",
    "# Finds  (((x − µj)^-1) (Σ(^-1)(x − µj))) value\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):\n",
    "    # Find the Gaussian radial basis functions  is φj (x)\n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "\n",
    "\n",
    "# GetPhiMatrix finds Gaussian radial basis functions  is φj (x) = exp (−1/2 (((x − µj)^-1) (Σ(^-1)(x − µj)))) \n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    #Datatranspose is transpose matrix of RawData\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))\n",
    "    # PHI is filled with zeros with dimensions of TrainingLen and MuMatrix \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))\n",
    "    # BigSigInv is the inverse of BigSigma of same dimensions 41*41\n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            # For M values of Mumatrix and 55699 values of Training, generate PHI\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    return PHI\n",
    "\n",
    "#We build the matrix for the basis function using the φj (x) formula\n",
    "#Returns an 55699*M matrix dimension\n",
    "TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pseudo inverse is found, because the matrix is not a square singular matrix. So  Moore-Penrose pseudo-inverse formula is applied \n",
    "# w∗ = (λI + (Φ^T)Φ)^(-1))(Φ^T)t\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    #I is created using the np.identity function as a M*M matrix.\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    #λI is determined for the I matrix of size M*M\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    # Φ^(-1) is found using np functional calls\n",
    "    # Finds the transpose of the Φ and has dimensions (M, 55699)\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    # (Φ^(-1)).Φ is determined\n",
    "    # (Φ^(-1)).Φ is reduced to a (M, M) matrix\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    # (λI + (Φ^T)Φ is calculated \n",
    "    # Again PHI_SQR_LI becomes (M, M) matrix\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    # (λI + (Φ^T)Φ ^(-1)\n",
    "    # PHI_SQR_INV is again a matrix of size (M, M)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    # (λI + (Φ^T)Φ)^(-1))(Φ^T)\n",
    "    # INTER becomes (M, 55699) because Φ^T is (M, 55699)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    # (λI + (Φ^T)Φ)^(-1))(Φ^T)t is determined\n",
    "    # w becomes (M,) because of the dot product being involved\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate weights for the training dataset\n",
    "W  = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda)) \n",
    "\n",
    "#We build the matrix for the basis function for test data using the φj (x) formula\n",
    "#Returns an 6961*M matrix dimension\n",
    "TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100)\n",
    "\n",
    "#We build the matrix for the basis function for validation data using the φj (x) formula\n",
    "#Returns an 6962*M matrix dimension\n",
    "VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100)\n",
    "\n",
    "#This method is the determining the target solution (f(x))\n",
    "#The target is the product of Weights and Phi \n",
    "#(N*M)x(M*1) = (N*1) matrix, is the required \n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    return Y\n",
    "\n",
    "#We calculate the f(x) solution for each Training, validation and test dataset\n",
    "TR_TEST_OUT  = GetValTest(TRAINING_PHI,W)\n",
    "VAL_TEST_OUT = GetValTest(VAL_PHI,W)\n",
    "TEST_OUT     = GetValTest(TEST_PHI,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This methods calculates the accuarcy of our prediction with the actual target.\n",
    "# Erms is the square root of the sum of the difference square, divided by total predictions \n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        # Compare the target value and predicted value\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "------------------LeToR Data------------------------\n",
      "----------------------------------------------------\n",
      "-------Closed Form with Radial Basis Function-------\n",
      "----------------------------------------------------\n",
      "M 4\n",
      "Lambda 0.02\n",
      "Accuracy Training   = 74.51300741485484\n",
      "Accuracy Validation = 75.15081873024992\n",
      "Accuracy Testing    = 70.23416175836805\n",
      "E_rms Training   = 0.5547953358069684\n",
      "E_rms Validation = 0.5427802627466416\n",
      "E_rms Testing    = 0.6327717807072879\n"
     ]
    }
   ],
   "source": [
    "# We determine the accuracy of our prediction model\n",
    "TrainingAccuracy   = str(GetErms(TR_TEST_OUT,TrainingTarget))\n",
    "ValidationAccuracy = str(GetErms(VAL_TEST_OUT,ValDataAct))\n",
    "TestAccuracy       = str(GetErms(TEST_OUT,TestDataAct))\n",
    "\n",
    "print ('UBITname      = davidjeg')\n",
    "print ('Person Number = 50290785')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"------------------LeToR Data------------------------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Closed Form with Radial Basis Function-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"M \" + str(M) + \"\\nLambda \" + str(C_Lambda))\n",
    "# Accuracy is stored in 0th posistion, and Erms is stored in the 1st position\n",
    "print (\"Accuracy Training   = \" + str(float(TrainingAccuracy.split(',')[0])))\n",
    "print (\"Accuracy Validation = \" + str(float(ValidationAccuracy.split(',')[0])))\n",
    "print (\"Accuracy Testing    = \" + str(float(TestAccuracy.split(',')[0])))\n",
    "print (\"E_rms Training   = \" + str(float(TrainingAccuracy.split(',')[1])))\n",
    "print (\"E_rms Validation = \" + str(float(ValidationAccuracy.split(',')[1])))\n",
    "print (\"E_rms Testing    = \" + str(float(TestAccuracy.split(',')[1])))\n",
    "\n",
    "# Overfitting causes increased accuracy in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Iteration: 1--------------\n",
      "---------Iteration: 2--------------\n",
      "---------Iteration: 3--------------\n",
      "---------Iteration: 4--------------\n",
      "---------Iteration: 5--------------\n",
      "---------Iteration: 6--------------\n",
      "---------Iteration: 7--------------\n",
      "---------Iteration: 8--------------\n",
      "---------Iteration: 9--------------\n",
      "---------Iteration: 10--------------\n",
      "---------Iteration: 11--------------\n",
      "---------Iteration: 12--------------\n",
      "---------Iteration: 13--------------\n",
      "---------Iteration: 14--------------\n",
      "---------Iteration: 15--------------\n",
      "---------Iteration: 16--------------\n",
      "---------Iteration: 17--------------\n",
      "---------Iteration: 18--------------\n",
      "---------Iteration: 19--------------\n",
      "---------Iteration: 20--------------\n",
      "---------Iteration: 21--------------\n",
      "---------Iteration: 22--------------\n",
      "---------Iteration: 23--------------\n",
      "---------Iteration: 24--------------\n",
      "---------Iteration: 25--------------\n",
      "---------Iteration: 26--------------\n",
      "---------Iteration: 27--------------\n",
      "---------Iteration: 28--------------\n",
      "---------Iteration: 29--------------\n",
      "---------Iteration: 30--------------\n",
      "---------Iteration: 31--------------\n",
      "---------Iteration: 32--------------\n",
      "---------Iteration: 33--------------\n",
      "---------Iteration: 34--------------\n",
      "---------Iteration: 35--------------\n",
      "---------Iteration: 36--------------\n",
      "---------Iteration: 37--------------\n",
      "---------Iteration: 38--------------\n",
      "---------Iteration: 39--------------\n",
      "---------Iteration: 40--------------\n",
      "---------Iteration: 41--------------\n",
      "---------Iteration: 42--------------\n",
      "---------Iteration: 43--------------\n",
      "---------Iteration: 44--------------\n",
      "---------Iteration: 45--------------\n",
      "---------Iteration: 46--------------\n",
      "---------Iteration: 47--------------\n",
      "---------Iteration: 48--------------\n",
      "---------Iteration: 49--------------\n",
      "---------Iteration: 50--------------\n",
      "---------Iteration: 51--------------\n",
      "---------Iteration: 52--------------\n",
      "---------Iteration: 53--------------\n",
      "---------Iteration: 54--------------\n",
      "---------Iteration: 55--------------\n",
      "---------Iteration: 56--------------\n",
      "---------Iteration: 57--------------\n",
      "---------Iteration: 58--------------\n",
      "---------Iteration: 59--------------\n",
      "---------Iteration: 60--------------\n",
      "---------Iteration: 61--------------\n",
      "---------Iteration: 62--------------\n",
      "---------Iteration: 63--------------\n",
      "---------Iteration: 64--------------\n",
      "---------Iteration: 65--------------\n",
      "---------Iteration: 66--------------\n",
      "---------Iteration: 67--------------\n",
      "---------Iteration: 68--------------\n",
      "---------Iteration: 69--------------\n",
      "---------Iteration: 70--------------\n",
      "---------Iteration: 71--------------\n",
      "---------Iteration: 72--------------\n",
      "---------Iteration: 73--------------\n",
      "---------Iteration: 74--------------\n",
      "---------Iteration: 75--------------\n",
      "---------Iteration: 76--------------\n",
      "---------Iteration: 77--------------\n",
      "---------Iteration: 78--------------\n",
      "---------Iteration: 79--------------\n",
      "---------Iteration: 80--------------\n",
      "---------Iteration: 81--------------\n",
      "---------Iteration: 82--------------\n",
      "---------Iteration: 83--------------\n",
      "---------Iteration: 84--------------\n",
      "---------Iteration: 85--------------\n",
      "---------Iteration: 86--------------\n",
      "---------Iteration: 87--------------\n",
      "---------Iteration: 88--------------\n",
      "---------Iteration: 89--------------\n",
      "---------Iteration: 90--------------\n",
      "---------Iteration: 91--------------\n",
      "---------Iteration: 92--------------\n",
      "---------Iteration: 93--------------\n",
      "---------Iteration: 94--------------\n",
      "---------Iteration: 95--------------\n",
      "---------Iteration: 96--------------\n",
      "---------Iteration: 97--------------\n",
      "---------Iteration: 98--------------\n",
      "---------Iteration: 99--------------\n",
      "---------Iteration: 100--------------\n",
      "---------Iteration: 101--------------\n",
      "---------Iteration: 102--------------\n",
      "---------Iteration: 103--------------\n",
      "---------Iteration: 104--------------\n",
      "---------Iteration: 105--------------\n",
      "---------Iteration: 106--------------\n",
      "---------Iteration: 107--------------\n",
      "---------Iteration: 108--------------\n",
      "---------Iteration: 109--------------\n",
      "---------Iteration: 110--------------\n",
      "---------Iteration: 111--------------\n",
      "---------Iteration: 112--------------\n",
      "---------Iteration: 113--------------\n",
      "---------Iteration: 114--------------\n",
      "---------Iteration: 115--------------\n",
      "---------Iteration: 116--------------\n",
      "---------Iteration: 117--------------\n",
      "---------Iteration: 118--------------\n",
      "---------Iteration: 119--------------\n",
      "---------Iteration: 120--------------\n",
      "---------Iteration: 121--------------\n",
      "---------Iteration: 122--------------\n",
      "---------Iteration: 123--------------\n",
      "---------Iteration: 124--------------\n",
      "---------Iteration: 125--------------\n",
      "---------Iteration: 126--------------\n",
      "---------Iteration: 127--------------\n",
      "---------Iteration: 128--------------\n",
      "---------Iteration: 129--------------\n",
      "---------Iteration: 130--------------\n",
      "---------Iteration: 131--------------\n",
      "---------Iteration: 132--------------\n",
      "---------Iteration: 133--------------\n",
      "---------Iteration: 134--------------\n",
      "---------Iteration: 135--------------\n",
      "---------Iteration: 136--------------\n",
      "---------Iteration: 137--------------\n",
      "---------Iteration: 138--------------\n",
      "---------Iteration: 139--------------\n",
      "---------Iteration: 140--------------\n",
      "---------Iteration: 141--------------\n",
      "---------Iteration: 142--------------\n",
      "---------Iteration: 143--------------\n",
      "---------Iteration: 144--------------\n",
      "---------Iteration: 145--------------\n",
      "---------Iteration: 146--------------\n",
      "---------Iteration: 147--------------\n",
      "---------Iteration: 148--------------\n",
      "---------Iteration: 149--------------\n",
      "---------Iteration: 150--------------\n",
      "---------Iteration: 151--------------\n",
      "---------Iteration: 152--------------\n",
      "---------Iteration: 153--------------\n",
      "---------Iteration: 154--------------\n",
      "---------Iteration: 155--------------\n",
      "---------Iteration: 156--------------\n",
      "---------Iteration: 157--------------\n",
      "---------Iteration: 158--------------\n",
      "---------Iteration: 159--------------\n",
      "---------Iteration: 160--------------\n",
      "---------Iteration: 161--------------\n",
      "---------Iteration: 162--------------\n",
      "---------Iteration: 163--------------\n",
      "---------Iteration: 164--------------\n",
      "---------Iteration: 165--------------\n",
      "---------Iteration: 166--------------\n",
      "---------Iteration: 167--------------\n",
      "---------Iteration: 168--------------\n",
      "---------Iteration: 169--------------\n",
      "---------Iteration: 170--------------\n",
      "---------Iteration: 171--------------\n",
      "---------Iteration: 172--------------\n",
      "---------Iteration: 173--------------\n",
      "---------Iteration: 174--------------\n",
      "---------Iteration: 175--------------\n",
      "---------Iteration: 176--------------\n",
      "---------Iteration: 177--------------\n",
      "---------Iteration: 178--------------\n",
      "---------Iteration: 179--------------\n",
      "---------Iteration: 180--------------\n",
      "---------Iteration: 181--------------\n",
      "---------Iteration: 182--------------\n",
      "---------Iteration: 183--------------\n",
      "---------Iteration: 184--------------\n",
      "---------Iteration: 185--------------\n",
      "---------Iteration: 186--------------\n",
      "---------Iteration: 187--------------\n",
      "---------Iteration: 188--------------\n",
      "---------Iteration: 189--------------\n",
      "---------Iteration: 190--------------\n",
      "---------Iteration: 191--------------\n",
      "---------Iteration: 192--------------\n",
      "---------Iteration: 193--------------\n",
      "---------Iteration: 194--------------\n",
      "---------Iteration: 195--------------\n",
      "---------Iteration: 196--------------\n",
      "---------Iteration: 197--------------\n",
      "---------Iteration: 198--------------\n",
      "---------Iteration: 199--------------\n",
      "---------Iteration: 200--------------\n",
      "---------Iteration: 201--------------\n",
      "---------Iteration: 202--------------\n",
      "---------Iteration: 203--------------\n",
      "---------Iteration: 204--------------\n",
      "---------Iteration: 205--------------\n",
      "---------Iteration: 206--------------\n",
      "---------Iteration: 207--------------\n",
      "---------Iteration: 208--------------\n",
      "---------Iteration: 209--------------\n",
      "---------Iteration: 210--------------\n",
      "---------Iteration: 211--------------\n",
      "---------Iteration: 212--------------\n",
      "---------Iteration: 213--------------\n",
      "---------Iteration: 214--------------\n",
      "---------Iteration: 215--------------\n",
      "---------Iteration: 216--------------\n",
      "---------Iteration: 217--------------\n",
      "---------Iteration: 218--------------\n",
      "---------Iteration: 219--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Iteration: 220--------------\n",
      "---------Iteration: 221--------------\n",
      "---------Iteration: 222--------------\n",
      "---------Iteration: 223--------------\n",
      "---------Iteration: 224--------------\n",
      "---------Iteration: 225--------------\n",
      "---------Iteration: 226--------------\n",
      "---------Iteration: 227--------------\n",
      "---------Iteration: 228--------------\n",
      "---------Iteration: 229--------------\n",
      "---------Iteration: 230--------------\n",
      "---------Iteration: 231--------------\n",
      "---------Iteration: 232--------------\n",
      "---------Iteration: 233--------------\n",
      "---------Iteration: 234--------------\n",
      "---------Iteration: 235--------------\n",
      "---------Iteration: 236--------------\n",
      "---------Iteration: 237--------------\n",
      "---------Iteration: 238--------------\n",
      "---------Iteration: 239--------------\n",
      "---------Iteration: 240--------------\n",
      "---------Iteration: 241--------------\n",
      "---------Iteration: 242--------------\n",
      "---------Iteration: 243--------------\n",
      "---------Iteration: 244--------------\n",
      "---------Iteration: 245--------------\n",
      "---------Iteration: 246--------------\n",
      "---------Iteration: 247--------------\n",
      "---------Iteration: 248--------------\n",
      "---------Iteration: 249--------------\n",
      "---------Iteration: 250--------------\n",
      "---------Iteration: 251--------------\n",
      "---------Iteration: 252--------------\n",
      "---------Iteration: 253--------------\n",
      "---------Iteration: 254--------------\n",
      "---------Iteration: 255--------------\n",
      "---------Iteration: 256--------------\n",
      "---------Iteration: 257--------------\n",
      "---------Iteration: 258--------------\n",
      "---------Iteration: 259--------------\n",
      "---------Iteration: 260--------------\n",
      "---------Iteration: 261--------------\n",
      "---------Iteration: 262--------------\n",
      "---------Iteration: 263--------------\n",
      "---------Iteration: 264--------------\n",
      "---------Iteration: 265--------------\n",
      "---------Iteration: 266--------------\n",
      "---------Iteration: 267--------------\n",
      "---------Iteration: 268--------------\n",
      "---------Iteration: 269--------------\n",
      "---------Iteration: 270--------------\n",
      "---------Iteration: 271--------------\n",
      "---------Iteration: 272--------------\n",
      "---------Iteration: 273--------------\n",
      "---------Iteration: 274--------------\n",
      "---------Iteration: 275--------------\n",
      "---------Iteration: 276--------------\n",
      "---------Iteration: 277--------------\n",
      "---------Iteration: 278--------------\n",
      "---------Iteration: 279--------------\n",
      "---------Iteration: 280--------------\n",
      "---------Iteration: 281--------------\n",
      "---------Iteration: 282--------------\n",
      "---------Iteration: 283--------------\n",
      "---------Iteration: 284--------------\n",
      "---------Iteration: 285--------------\n",
      "---------Iteration: 286--------------\n",
      "---------Iteration: 287--------------\n",
      "---------Iteration: 288--------------\n",
      "---------Iteration: 289--------------\n",
      "---------Iteration: 290--------------\n",
      "---------Iteration: 291--------------\n",
      "---------Iteration: 292--------------\n",
      "---------Iteration: 293--------------\n",
      "---------Iteration: 294--------------\n",
      "---------Iteration: 295--------------\n",
      "---------Iteration: 296--------------\n",
      "---------Iteration: 297--------------\n",
      "---------Iteration: 298--------------\n",
      "---------Iteration: 299--------------\n",
      "0.7250446381329284\n",
      "----------Gradient Descent Solution--------------------\n",
      "Learning rate 0.05783149319662402\n",
      "E_rms Training   = 0.56545\n",
      "E_rms Validation = 0.55427\n",
      "E_rms Testing    = 0.63633\n",
      "Accuracy Training   = 70.23416175836805\n",
      "Accuracy Validation = 70.23416175836805\n",
      "Accuracy Testing    = 70.23416175836805\n"
     ]
    }
   ],
   "source": [
    "# The current weight. Initially it is extended from W of closed solution. (W*200 in this case)\n",
    "W_Now        = np.dot(220, W)\n",
    "\n",
    "# Lambda is the regularization parameter, added to avoid overfitting\n",
    "La           = 2\n",
    "\n",
    "#The Erms is initialized for Train, validation and test  dataset\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "\n",
    "Acc_L_Erms_Val   = []\n",
    "Acc_L_Erms_TR    = []\n",
    "Acc_L_Erms_Test  = []\n",
    "\n",
    "f = []\n",
    "g = []\n",
    "\n",
    "#maximum number of iterations. We consider 300 as the range\n",
    "#The dimensions of all the steps are (M,1) because of the dimension of W\n",
    "for i in range(1,300):\n",
    "    f.append(i)\n",
    "    learningRate = 1/ math.sqrt(i)    \n",
    "    print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    #Delta_E_D is -(t -(w^T)Φ(x))Φ(x)\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "    #La_Delta is λW\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    #Delta_E is the addition of the two previous values\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "    # - ve sign, because we need to descent for minimizing error value\n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    #W_T_Next and and W_Now are updated with the current values for the next iteration\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "    \n",
    "    #ValTest is the determining the target solution (f(x))\n",
    "    #The target is the product of Weights and Phi \n",
    "    TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "    VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "    TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "\n",
    "    # This methods calculates the accuarcy of our prediction with the actual target.\n",
    "    # Erms is the square root of the sum of the difference square, divided by total predictions \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "    Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "\n",
    "    #Save the value of Erms\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "    g.append(float(Erms_Test.split(',')[1]))\n",
    "    Acc_L_Erms_TR.append(float(Erms_TR.split(',')[0]))\n",
    "    Acc_L_Erms_Val.append(float(Erms_Val.split(',')[0]))\n",
    "    Acc_L_Erms_Test.append(float(Erms_Test.split(',')[0]))\n",
    "\n",
    "print(float(Erms_Test.split(',')[1]))\n",
    "print ('----------Gradient Descent Solution--------------------')\n",
    "print (\"Learning rate \" + str(learningRate))\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "\n",
    "print (\"Accuracy Training   = \" + str(float(Erms_Test.split(',')[0])))\n",
    "print (\"Accuracy Validation = \" + str(float(Erms_Test.split(',')[0])))\n",
    "print (\"Accuracy Testing    = \" + str(float(Erms_Test.split(',')[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5ea3db9198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution--------------------\n",
      "M = 15 \n",
      "Lambda  = 0.0001\n",
      "eta=0.01\n",
      "E_rms Training   = 0.56545\n",
      "E_rms Validation = 0.55427\n",
      "E_rms Testing    = 0.63633\n",
      "Accuracy Training   = 74.52198423670085\n",
      "Accuracy Validation = 75.17954610744039\n",
      "Accuracy Testing    = 70.23416175836805\n"
     ]
    }
   ],
   "source": [
    "# The other graphs in the report were generated on a similar fashion.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(f, g, 'ro')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Erms')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print ('----------Gradient Descent Solution--------------------')\n",
    "print (\"M = 15 \\nLambda  = 0.0001\\neta=0.01\")\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "print (\"Accuracy Training   = \" + str(float(Erms_TR.split(',')[0])))\n",
    "print (\"Accuracy Validation = \" + str(float(Erms_Val.split(',')[0])))\n",
    "print (\"Accuracy Testing    = \" + str(float(Erms_Test.split(',')[0])))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
